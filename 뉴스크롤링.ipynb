{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "뉴스크롤링.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrnulGTWPdn+Zjqm/+WgNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/980608/python-project/blob/main/%EB%89%B4%EC%8A%A4%ED%81%AC%EB%A1%A4%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpDXqljoag22"
      },
      "source": [
        "#  Daum.net 뉴스..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOTljp1adpIX",
        "outputId": "e4717345-af07-47f7-d7f9-bd836242f304"
      },
      "source": [
        "! pip install bs4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRIANL6Uc4yv"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs    #데이터 정리\n",
        "import urllib.request as ur            #인터넷 페이지 연결"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpzx2xxdxx6"
      },
      "source": [
        "news = 'https://news.daum.net/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L91xAW1Kd8S9"
      },
      "source": [
        "soup = bs(ur.urlopen(news).read(),'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPy7E09Jed0m"
      },
      "source": [
        "soup.find_all('div',{'class' : 'item_issue'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nojh9Z4GgLfs"
      },
      "source": [
        "for i in soup.find_all('div',{'class' : 'item_issue'}):\n",
        "  print(i.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVkDiJ8Lg-59"
      },
      "source": [
        "soup.find_all('a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5tKXAY7iJVD",
        "outputId": "b0a2f666-651d-4e9c-c69a-602c375d7597"
      },
      "source": [
        "soup.find_all('a')[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<a href=\"#kakaoBody\">본문 바로가기</a>,\n",
              " <a href=\"#kakaoGnb\">메뉴 바로가기</a>,\n",
              " <a data-tiara-layer=\"GNB default service_news\" href=\"https://news.daum.net/\" id=\"kakaoServiceLogo\"><span class=\"ir_wa\">뉴스</span></a>,\n",
              " <a class=\"link_services link_services1\" data-tiara-layer=\"service_enter\" href=\"https://entertain.daum.net\"><span class=\"ir_wa\">연예</span></a>,\n",
              " <a class=\"link_services link_services2\" data-tiara-layer=\"service_sports\" href=\"https://sports.daum.net\"><span class=\"ir_wa\">스포츠</span></a>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-MMHoVQiWMB",
        "outputId": "95cd2542-2fb2-43fc-fbc1-bcd69a6a3643"
      },
      "source": [
        "for i in soup.find_all('a')[:5]:\n",
        "  print(i.get('href')) # href 값만 뽑아와라\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#kakaoBody\n",
            "#kakaoGnb\n",
            "https://news.daum.net/\n",
            "https://entertain.daum.net\n",
            "https://sports.daum.net\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_89cPSd8iypg"
      },
      "source": [
        "news = 'https://news.daum.net/'\n",
        "\n",
        "soup = bs(ur.urlopen(news).read(),'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwgSXM79jNhT"
      },
      "source": [
        "for i in soup.find_all('div',{'class':'item_issue'}):\n",
        "  i.find_all('a').get('href')  #find_all() 결과값이 리스트형이기 때문에 에러 발생"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0VVcrj5kGmV",
        "outputId": "312725a0-ff34-423c-c53f-2800117411c7"
      },
      "source": [
        "for i in soup.find_all('div',{'class':'item_issue'}):\n",
        "  print(i.find_all('a')[0].get('href'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://news.v.daum.net/v/20211111193200879\n",
            "https://news.v.daum.net/v/20211111193011845\n",
            "https://news.v.daum.net/v/20211111191928610\n",
            "https://news.v.daum.net/v/20211111161500425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqHa5XrukPTk"
      },
      "source": [
        "# 1. soup 객체에서 class값이 item_issue 인 <div>를 find_all 가져온다.\n",
        "# 2. <div> 객체에서 <a>태그들을 Find_all 로 가져온다.\n",
        "# 3. <a>태그의 href 속성의 밸류값을 get 출력한다.\n",
        "\n",
        "#  find_all은 태그를 추출  GET은 태그 안의 속성값을 추출"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg5qAPQFlviz"
      },
      "source": [
        "# naver.com 뉴스 검색..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "ESp3c7bclx4P",
        "outputId": "5aade024-e443-4e41-9085-329c3f428ae2"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs    #데이터 정리\n",
        "import urllib.request as ur            #인터넷 페이지 연결\n",
        "import requests\n",
        "from pandas import Data\n",
        "\n",
        "#  input 검색할 키워드 설정\n",
        "query = input('검색 키워드를 입력하세요 : ')\n",
        "#  검색할 뉴스 갯수 설정\n",
        "news_num = int(input('총 뉴스기사의 수를 입력해 주세요(숫자로 입력) : '))\n",
        "#  요청할 URL\n",
        "news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
        "\n",
        "req = requests.get(news_url.format(query))\n",
        "soup = bs(req.text,'html.parser')\n",
        "\n",
        "# 원하는 정보를 추출해서 담을 변수 생성(딕셔너리)\n",
        "news_dict = {}\n",
        "# KEY 번호 Value 뉴스 기사 정보\n",
        "idx = 0\n",
        "# 현재 페이지, 기사의 수가 1페이지를 넘어가는 경우가 발생 가능\n",
        "cur_page = 1\n",
        "\n",
        "import re\n",
        "\n",
        "while idx < news_num:\n",
        "  table = soup.find('ul',{'class':'list_news'})\n",
        "  li_list = table.find_all('li',{'id':re.compile('sp_nws.*')})\n",
        "  area_list = [li.find('div',{'class':'news_area'}) for li in li_list]\n",
        "  a_list[area.find('a',{'class':'news_tit'}) for area in area_list]\n",
        "\n",
        "  for n in a_list[:min(len(a_list).news_num-idx)]:\n",
        "    news_dict[idx] = {'title': n.get('title'),\n",
        "                      'url': n.get('href')}\n",
        "\n",
        "    idx +=1\n",
        "  cur_page += 1\n",
        "\n",
        "  pages = soup.find('div',{'class': 'sc_page_inner'})\n",
        "\n",
        "  next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
        "\n",
        "  req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
        "  soup = soup = bs(req.text,'html.parser')\n",
        "\n",
        "  print('크롤링 완료')\n",
        "  print('데이;터 프레임으로 변환')\n",
        "  news_df = DataFrame(news_dict).T\n",
        "\n",
        "  news_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-e06e0cc5a4a9>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    a_list[area.find('a',{'class':'news_tit'}) for area in area_list:]\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDsjqRmaqvlj"
      },
      "source": [
        "# 원하는 정보를 추출해서 담을 변수 생성(딕셔너리)\n",
        "news_dict = {}\n",
        "# KEY 번호 Value 뉴스 기사 정보\n",
        "idx = 0\n",
        "# 현재 페이지, 기사의 수가 1페이지를 넘어가는 경우가 발생 가능\n",
        "cur_page = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch8CNBX6r0Ln"
      },
      "source": [
        "import re\n",
        "\n",
        "while idx < news_num:\n",
        "  table = soup.find('ul',{'class':'list_news'})\n",
        "  li_list = table.find_all('li',{'id':re.compile('sp_nws.*')})\n",
        "  area_list = [li,find('div',{'class':'news_area'}) for li in li_list]\n",
        "  a_list[area,find('a',{'class':'news_tit'}) for area in area_list]\n",
        "\n",
        "  for n in a_list[:min(len(a_list).news_num-idx)]:\n",
        "    news_dict[idx] = {'title':n.get('title'),\n",
        "                      'url':n.get('href')}\n",
        "\n",
        "    idx +=1\n",
        "  cur_page += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Sa6TdGyfIP",
        "outputId": "5bc37199-98bb-4a3f-bd21-10b5a6895291"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "\n",
        "query = input('검색 키워드를 입력하세요 : ')\n",
        "news_num = int(input('총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : '))\n",
        "query = query.replace(' ', '+')\n",
        "\n",
        "\n",
        "news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'\n",
        "\n",
        "req = requests.get(news_url.format(query))\n",
        "soup = bs(req.text, 'html.parser')\n",
        "\n",
        "\n",
        "news_dict = {}\n",
        "idx = 1\n",
        "cur_page = 1\n",
        "\n",
        "print()\n",
        "print('크롤링 중...')\n",
        "\n",
        "while idx < news_num:\n",
        "\n",
        "    \n",
        "    table = soup.find('ul',{'class' : 'list_news'})\n",
        "    li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})\n",
        "    area_list = [li.find('div', {'class' : 'news_area'}) for li in li_list]\n",
        "    a_list = [area.find('a', {'class' : 'news_tit'}) for area in area_list]\n",
        "    \n",
        "    for n in a_list[:min(len(a_list), news_num-idx)]:\n",
        "        news_dict[idx] = {'title' : n.get('title'),\n",
        "                          'url' : n.get('href') }\n",
        "        idx += 1\n",
        "\n",
        "    cur_page += 1\n",
        "\n",
        "    pages = soup.find('div', {'class' : 'sc_page_inner'})\n",
        "    next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')\n",
        "    \n",
        "    req = requests.get('https://search.naver.com/search.naver' + next_page_url)\n",
        "    soup = bs(req.text, 'html.parser')\n",
        "\n",
        "print('크롤링 완료')\n",
        "\n",
        "print('데이터프레임 변환')\n",
        "news_df = DataFrame(news_dict).T\n",
        "\n",
        "\n",
        "file_name = '네이버 뉴스_{}.xlsx'.format(query)\n",
        "news_df.to_excel(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색 키워드를 입력하세요 : 코로나\n",
            "총 필요한 뉴스기사 수를 입력해주세요(숫자만 입력) : 50\n",
            "\n",
            "크롤링 중...\n",
            "크롤링 완료\n",
            "데이터프레임 변환\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOXMXn0z0B8v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}