{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "네이버 영화 장르 분석.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOj9pFpUbgnLmbTF3gJ6azL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/980608/python-project/blob/main/%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%98%81%ED%99%94_%EC%9E%A5%EB%A5%B4_%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWw0BWKR_2S2"
      },
      "outputs": [],
      "source": [
        "개인 프로젝트를 진행(4.12~13)\n",
        "나이브베이징을 사용하여 분석을 해보자\n",
        "네이버 영화 장르 분석\n",
        "from pandas import Series,DataFrame\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from urllib.error import URLError, HTTPError\n",
        "from fake_useragent import UserAgent\n",
        "from urllib.request import urlopen\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from konlpy.tag import Okt\n",
        "import operator\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "url = 'https://movie.naver.com/movie/sdb/browsing/bmovie_genre.naver'\n",
        "driver =  webdriver.Chrome('c:/data/chromedriver.exe')\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(2)\n",
        "\n",
        "movie = DataFrame(columns=['name','genre','text'])\n",
        "failed_url = []\n",
        "\n",
        "# 판타지 / 공포\n",
        "for i in range(2,5,2):\n",
        "    driver.find_element(By.CSS_SELECTOR,'table.directory_item_other > tbody > tr:nth-of-type(1) > td:nth-of-type('+str(i)+') > a').click() # 장르 선택\n",
        "    \n",
        "    for u in range(2,7): # 페이지 이동 for문\n",
        "        for j in range(1,21): # 자료 수집 for문     \n",
        "            try:\n",
        "                driver.find_element(By.CSS_SELECTOR,'ul.directory_list > li:nth-of-type('+str(j)+') > a').click() # 제목클릭\n",
        "                html = driver.page_source\n",
        "                soup = bs(html,'html.parser')\n",
        "                time.sleep(2)\n",
        "                name = soup.select_one('h3.h_movie > a').text # 제목\n",
        "                genre = soup.select_one('dl.info_spec > dd > p > span > a:nth-of-type(1)').text # 장르\n",
        "                text = soup.select_one('p.con_tx').text # 줄거리\n",
        "                movie = movie.append({'name' : name,'genre':genre,'text':text},ignore_index = True)\n",
        "                time.sleep(2)\n",
        "                driver.back() # 뒤로가기\n",
        "                time.sleep(2)\n",
        "                \n",
        "            except:\n",
        "                failed_url.append(i) # 오류 페이지 저장\n",
        "                driver.back()\n",
        "        \n",
        "        if u == 2:\n",
        "            driver.find_element(By.CSS_SELECTOR,'div.pagenavigation > table > tbody > tr > td:nth-of-type('+str(u)+') > a').click()\n",
        "        else:\n",
        "            driver.find_element(By.CSS_SELECTOR,'div.pagenavigation > table > tbody > tr > td:nth-of-type('+str(u+1)+') > a').click()\n",
        "\n",
        "    driver.find_element(By.CSS_SELECTOR,'div.tab_type_6 > ul > li:nth-of-type(4)').click() # 장르 페이지로 가기\n",
        "    time.sleep(2)\n",
        "\n",
        "\n",
        "movie.to_csv('c:/data/movie_1.csv',index=False)\n",
        "movie_1 = pd.read_csv('c:/data/movie_1.csv')\n",
        "\n",
        "# 멜로, 스릴\n",
        "for i in range(1,4,2):\n",
        "    driver.find_element(By.CSS_SELECTOR,'table.directory_item_other > tbody > tr:nth-of-type(2) > td:nth-of-type('+str(i)+') > a').click() # 장르 선택\n",
        "    \n",
        "    for u in range(2,7): # 페이지 이동 for문\n",
        "        for j in range(1,21): # 자료 수집 for문     \n",
        "            try:\n",
        "                driver.find_element(By.CSS_SELECTOR,'ul.directory_list > li:nth-of-type('+str(j)+') > a').click() # 제목클릭\n",
        "                html = driver.page_source\n",
        "                soup = bs(html,'html.parser')\n",
        "                time.sleep(2)\n",
        "                name = soup.select_one('h3.h_movie > a').text # 제목\n",
        "                genre = soup.select_one('dl.info_spec > dd > p > span > a:nth-of-type(1)').text # 장르\n",
        "                text = soup.select_one('p.con_tx').text # 줄거리\n",
        "                movie_1 = movie_1.append({'name' : name,'genre':genre,'text':text},ignore_index = True)\n",
        "                time.sleep(2)\n",
        "                driver.back() # 뒤로가기\n",
        "                time.sleep(2)\n",
        "                \n",
        "            except:\n",
        "                failed_url.append(i) # 오류 페이지 저장\n",
        "                driver.back()\n",
        "        \n",
        "        if u == 2:\n",
        "            driver.find_element(By.CSS_SELECTOR,'div.pagenavigation > table > tbody > tr > td:nth-of-type('+str(u)+') > a').click()\n",
        "        else:\n",
        "            driver.find_element(By.CSS_SELECTOR,'div.pagenavigation > table > tbody > tr > td:nth-of-type('+str(u+1)+') > a').click()\n",
        "\n",
        "\n",
        "    #driver.find_element(By.CSS_SELECTOR,'ul.navi_sub > li:nth-of-type(2) > a').click()\n",
        "    driver.find_element(By.CSS_SELECTOR,'div.tab_type_6 > ul > li:nth-of-type(4)').click() # 장르 페이지로 가기\n",
        "    time.sleep(2)\n",
        "\n",
        "\n",
        "# 다큐멘터리, 가족\n",
        "for i in range(2,5,2):\n",
        "    driver.find_element(By.CSS_SELECTOR,'table.directory_item_other > tbody > tr:nth-of-type(3) > td:nth-of-type('+str(i)+') > a').click() # 장르 선택\n",
        "    \n",
        "    for u in range(2,7): # 페이지 이동 for문\n",
        "        for j in range(1,21): # 자료 수집 for문     \n",
        "            try:\n",
        "                driver.find_element(By.CSS_SELECTOR,'ul.directory_list > li:nth-of-type('+str(j)+') > a').click() # 제목클릭\n",
        "                html = driver.page_source\n",
        "                soup = bs(html,'html.parser')\n",
        "                time.sleep(2)\n",
        "                name = soup.select_one('h3.h_movie > a').text # 제목\n",
        "                genre = soup.select_one('dl.info_spec > dd > p > span > a:nth-of-type(1)').text # 장르\n",
        "                text = soup.select_one('p.con_tx').text # 줄거리\n",
        "                movie_1 = movie_1.append({'name' : name,'genre':genre,'text':text},ignore_index = True)\n",
        "                time.sleep(2)\n",
        "                driver.back() # 뒤로가기\n",
        "                time.sleep(2)\n",
        "                \n",
        "            except:\n",
        "                failed_url.append(i) # 오류 페이지 저장\n",
        "                driver.back()\n",
        "        \n",
        "        if u == 2:\n",
        "            driver.find_element(By.CSS_SELECTOR,'div.pagenavigation > table > tbody > tr > td:nth-of-type('+str(u)+') > a').click()\n",
        "        else:\n",
        "            driver.find_element(By.CSS_SELECTOR,'div.pagenavigation > table > tbody > tr > td:nth-of-type('+str(u+1)+') > a').click()\n",
        "\n",
        "\n",
        "    #driver.find_element(By.CSS_SELECTOR,'ul.navi_sub > li:nth-of-type(2) > a').click()\n",
        "    driver.find_element(By.CSS_SELECTOR,'div.tab_type_6 > ul > li:nth-of-type(4)').click() # 장르 페이지로 가기\n",
        "    time.sleep(2)\n",
        "\n",
        "movie_1.to_csv('c:/data/naver_movie.csv',index=False)\n",
        "movie = pd.read_csv('c:/data/naver_movie.csv')\n",
        "\n",
        "movie[movie['text'].isnull() != False]\n",
        "movie = movie.drop(305, axis=0)\n",
        "movie.to_csv('c:/data/naver_movie.csv',index=False)\n",
        "movie = pd.read_csv('c:/data/naver_movie.csv')\n",
        "\n",
        "\n",
        "# 1차 전처리 작업\n",
        "movie['text'][0]\n",
        "re.findall('\\d+\\w',movie['text'][0])\n",
        "\n",
        "re.sub('\\(\\w+\\)',' ',movie['text'][0])\n",
        "\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\xa0',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\n|\\t+',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\{“I Think, Therefore I am \\(나는 생각한다, 고로 존재한다\\)” - Descartes \\(데가르트\\), 1596-1650\\}',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\s{2,}',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\(.+?\\)',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : x.strip())\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\(.+?\\)',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('[,‘’.]',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\d+\\w',' ',x))\n",
        "\n",
        "movie.to_csv('c:/data/text_movie.csv',index=False)\n",
        "movie = pd.read_csv('c:/data/text_movie.csv')\n",
        "\n",
        "movie['text'][21]\n",
        "re.findall(\"['|']\",movie['text'][8])\n",
        "\n",
        "re.sub('[\"\"]','',movie['text'][13])\n",
        "\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\[.+?\\]',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('\\s{2,}',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : x.strip())\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('…',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('[”|-|?|<|>|!]',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('MEM:',' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub(\"['|']\",' ',x))\n",
        "movie['text'] = movie['text'].apply(lambda x : re.sub('[\"\"]','',x))\n",
        "\n",
        "\n",
        "\n",
        "okt_pos(movie_1['text'][0])\n",
        "\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "def okt_pos(arg):\n",
        "    token_cpr = []\n",
        "    for i in okt.pos(arg):\n",
        "        if i[1] in ['Noun','Adjective']:\n",
        "            token_cpr.append(i[0])\n",
        "    token_cpr = [j for j in token_cpr if len(j) >= 2]\n",
        "    return token_cpr\n",
        "tokenizer = okt_pos\n",
        "\n",
        "x_train,x_test,y_train,y_test= train_test_split(movie['text'],movie['genre'],test_size = 0.2)\n",
        "\n",
        "cv = CountVectorizer(tokenizer = okt_pos)\n",
        "cv = CountVectorizer(ngram_range=(2,2))\n",
        "x_train = cv.fit_transform(x_train)\n",
        "cv.get_feature_names()\n",
        "x_train.toarray()\n",
        "\n",
        "x_test = cv.transform(x_test)\n",
        "x_test.toarray()\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(x_train,y_train)\n",
        "\n",
        "y_predict = nb.predict(x_test)\n",
        "accuracy_score(y_test,y_predict)\n",
        "\n",
        "■ 혼동행렬(confusion matrix)\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "confusion_matrix(y_test,y_predict)\n",
        "print(classification_report(y_test,y_predict))"
      ]
    }
  ]
}